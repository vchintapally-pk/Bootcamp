{"paragraphs":[{"text":"# Agenda\n* Zeppelin Setup\n* Spark Setup\n* Run sample code in spark in scala & Py-Spark","user":"anonymous","dateUpdated":"2019-07-23T21:16:23-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Agenda</h1>\n<ul>\n  <li>Spark Setup</li>\n  <li>Zeppelin Setup</li>\n  <li>Run sample code in spark in scala &amp; Py-Spark</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1563932720658_-1230712660","id":"20190723-204520_1148756891","dateCreated":"2019-07-23T20:45:20-0500","dateStarted":"2019-07-23T20:47:05-0500","dateFinished":"2019-07-23T20:47:05-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1516"},{"text":"## Zeppelin Setup\n\n* Download Zeppelin from https://zeppelin.apache.org/download.html , Download zeppelin-0.8.1-bin-all\n* Extract ``` tar -xvf zeppelin-0.8.1-bin-all.tgz ```\n* Place it in home folder ```mv zeppelin-0.8.1-bin-all ../zeppelin ```\n* Define ZEPPELIN_HOME and add it to PATH variables\n\n```\necho \"\nexport ZEPPELIN_HOME=/home/vektor/zeppelin/\nexport PATH=$PATH:$ZEPPELIN_HOME/bin/\n\" >> ~/.bashrc\n\nsource ~/.bashrc\n```\n\n* Start Zeppelin as \n\n``` \nvektor@pop-os:~$ zeppelin-daemon.sh start\nZeppelin start                                             [  OK  ]\n```\n* Navigate to http://localhost:8080/#/","user":"anonymous","dateUpdated":"2019-07-23T21:16:30-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Zeppelin Setup</h2>\n<ul>\n  <li>Download Zeppelin from <a href=\"https://zeppelin.apache.org/download.html\">https://zeppelin.apache.org/download.html</a> , Download zeppelin-0.8.1-bin-all</li>\n  <li>Extract <code>tar -xvf zeppelin-0.8.1-bin-all.tgz</code></li>\n  <li>Place it in home folder <code>mv zeppelin-0.8.1-bin-all ../zeppelin</code></li>\n  <li>Define ZEPPELIN_HOME and add it to PATH variables</li>\n</ul>\n<pre><code>echo &quot;\nexport ZEPPELIN_HOME=/home/vektor/zeppelin/\nexport PATH=$PATH:$ZEPPELIN_HOME/bin/\n&quot; &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n</code></pre>\n<ul>\n  <li>Start Zeppelin as</li>\n</ul>\n<pre><code>vektor@pop-os:~$ zeppelin-daemon.sh start\nZeppelin start                                             [  OK  ]\n</code></pre>\n<ul>\n  <li>Navigate to <a href=\"http://localhost:8080/#/\">http://localhost:8080/#/</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1563930400277_-1930766589","id":"20190723-200640_337802475","dateCreated":"2019-07-23T20:06:40-0500","dateStarted":"2019-07-23T21:16:30-0500","dateFinished":"2019-07-23T21:16:30-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1517"},{"text":"## Spark Setup\n\n* Download Spark from https://spark.apache.org/downloads.html , Select 2.4.3 release, \n* Extract ```tar -xvf spark-2.4.3-bin-hadoop2.7.tgz ```\n* Place it in home folder ```mv spark-2.4.3-bin-hadoop2.7 ../spark ```\n* Define SPARK_HOME and add it to PATH variables \n\n```\necho \"\nexport SPARK_HOME=/home/vektor/spark/\nexport PATH=$PATH:$SPARK_HOME/bin/\n\" >> ~/.bashrc\n\nsource ~/.bashrc\n```\n\n","user":"anonymous","dateUpdated":"2019-07-23T21:00:55-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark Setup</h2>\n<ul>\n  <li>Download Spark from <a href=\"https://spark.apache.org/downloads.html\">https://spark.apache.org/downloads.html</a> , Select 2.4.3 release,</li>\n  <li>Extract <code>tar -xvf spark-2.4.3-bin-hadoop2.7.tgz</code></li>\n  <li>Place it in home folder <code>mv spark-2.4.3-bin-hadoop2.7 ../spark</code></li>\n  <li>Define SPARK_HOME and add it to PATH variables</li>\n</ul>\n<pre><code>echo &quot;\nexport SPARK_HOME=/home/vektor/spark/\nexport PATH=$PATH:$SPARK_HOME/bin/\n&quot; &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563928812083_2131244599","id":"20190723-194012_1146780741","dateCreated":"2019-07-23T19:40:12-0500","dateStarted":"2019-07-23T20:42:42-0500","dateFinished":"2019-07-23T20:42:42-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1518"},{"text":"## Run Sample Spark Application\n### Using Spark-Shell\n\n\n```\nvektor@pop-os:~$ spark-shell\n19/07/23 19:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nSpark context Web UI available at http://192.168.0.22:4040\nSpark context available as 'sc' (master = local[*], app id = local-1563929660211).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n         \nUsing Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> val NUM_SAMPLES=1000000\nNUM_SAMPLES: Int = 1000000\n\nscala> val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>\n     |   val x = math.random\n     |   val y = math.random\n     |   x*x + y*y < 1\n     | }.count()\ncount: Long = 785340                                                            \n\nprintln(\"Pi is roughly \"+{4.0 * count / NUM_SAMPLES})\nPi is roughly 3.14136\n```\n\n### Using Spark-Submit\n```\nvektor@pop-os:~$ spark-submit --class org.apache.spark.examples.SparkPi ~/spark/examples/jars/spark-examples_2.11-2.4.3.jar 1000\n.......................................\n.......................................\nPi is roughly 3.14176503141765\n19/07/23 20:05:18 INFO SparkUI: Stopped Spark web UI at http://192.168.0.22:4040\n19/07/23 20:05:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n19/07/23 20:05:18 INFO MemoryStore: MemoryStore cleared\n19/07/23 20:05:18 INFO BlockManager: BlockManager stopped\n19/07/23 20:05:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n19/07/23 20:05:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n19/07/23 20:05:18 INFO SparkContext: Successfully stopped SparkContext\n19/07/23 20:05:18 INFO ShutdownHookManager: Shutdown hook called\n19/07/23 20:05:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-a6a71a9f-ea0c-43f6-bd00-a631e55dfb06\n19/07/23 20:05:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2f66555-7953-4b13-9ab1-3d83f22db302\nvektor@pop-os:~$ \n```","user":"anonymous","dateUpdated":"2019-07-23T21:02:28-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorHide":false,"tableHide":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{"4.0 * count / NUM_SAMPLES":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Run Sample Spark Application</h2>\n<h3>Using Spark-Shell</h3>\n<pre><code>vektor@pop-os:~$ spark-shell\n19/07/23 19:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nSpark context Web UI available at http://192.168.0.22:4040\nSpark context available as &#39;sc&#39; (master = local[*], app id = local-1563929660211).\nSpark session available as &#39;spark&#39;.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  &#39;_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n         \nUsing Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; val NUM_SAMPLES=1000000\nNUM_SAMPLES: Int = 1000000\n\nscala&gt; val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =&gt;\n     |   val x = math.random\n     |   val y = math.random\n     |   x*x + y*y &lt; 1\n     | }.count()\ncount: Long = 785340                                                            \n\nprintln(&quot;Pi is roughly &quot;+{4.0 * count / NUM_SAMPLES})\nPi is roughly 3.14136\n</code></pre>\n<h3>Using Spark-Submit</h3>\n<pre><code>vektor@pop-os:~$ spark-submit --class org.apache.spark.examples.SparkPi ~/spark/examples/jars/spark-examples_2.11-2.4.3.jar 1000\n.......................................\n.......................................\nPi is roughly 3.14176503141765\n19/07/23 20:05:18 INFO SparkUI: Stopped Spark web UI at http://192.168.0.22:4040\n19/07/23 20:05:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n19/07/23 20:05:18 INFO MemoryStore: MemoryStore cleared\n19/07/23 20:05:18 INFO BlockManager: BlockManager stopped\n19/07/23 20:05:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n19/07/23 20:05:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n19/07/23 20:05:18 INFO SparkContext: Successfully stopped SparkContext\n19/07/23 20:05:18 INFO ShutdownHookManager: Shutdown hook called\n19/07/23 20:05:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-a6a71a9f-ea0c-43f6-bd00-a631e55dfb06\n19/07/23 20:05:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2f66555-7953-4b13-9ab1-3d83f22db302\nvektor@pop-os:~$ \n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563928881971_1729220797","id":"20190723-194121_1675274886","dateCreated":"2019-07-23T19:41:21-0500","dateStarted":"2019-07-23T20:20:37-0500","dateFinished":"2019-07-23T20:20:37-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1519"},{"text":"### test Py-Spark\n* Run pyspark shell as\n\n```\nvektor@pop-os:~$ pyspark \n```\n* Let's Run some code\n\n```\nvektor@pop-os:~$ pyspark \nPython 2.7.15+ (default, Nov 27 2018, 23:36:35) \n[GCC 7.3.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n\nUsing Python version 2.7.15+ (default, Nov 27 2018 23:36:35)\nSparkSession available as 'spark'.\n>>> import random\n>>> NUM_SAMPLES=1000000\n>>> def inside(p):\n...     x, y = random.random(), random.random()\n...     return x*x + y*y < 1\n... \n>>> count = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n...              .filter(inside).count()\n>>> print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)                      \nPi is roughly 3.145056\n>>> \n\n```\n\n","user":"anonymous","dateUpdated":"2019-07-23T21:05:27-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>test Py-Spark</h3>\n<ul>\n  <li>Run pyspark shell as</li>\n</ul>\n<pre><code>vektor@pop-os:~$ pyspark \n</code></pre>\n<ul>\n  <li>Let&rsquo;s Run some code</li>\n</ul>\n<pre><code>vektor@pop-os:~$ pyspark \nPython 2.7.15+ (default, Nov 27 2018, 23:36:35) \n[GCC 7.3.0] on linux2\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\nUsing Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  &#39;_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n\nUsing Python version 2.7.15+ (default, Nov 27 2018 23:36:35)\nSparkSession available as &#39;spark&#39;.\n&gt;&gt;&gt; import random\n&gt;&gt;&gt; NUM_SAMPLES=1000000\n&gt;&gt;&gt; def inside(p):\n...     x, y = random.random(), random.random()\n...     return x*x + y*y &lt; 1\n... \n&gt;&gt;&gt; count = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n...              .filter(inside).count()\n&gt;&gt;&gt; print &quot;Pi is roughly %f&quot; % (4.0 * count / NUM_SAMPLES)                      \nPi is roughly 3.145056\n&gt;&gt;&gt; \n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563929426420_232385500","id":"20190723-195026_42540641","dateCreated":"2019-07-23T19:50:26-0500","dateStarted":"2019-07-23T20:20:59-0500","dateFinished":"2019-07-23T20:20:59-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1520"},{"text":"### Py-Spark - Spark Submit\n```\nvektor@pop-os:~$ spark-submit spark/examples/src/main/python/pi.py 1000\n..............................................................................\n..............................................................................\nPi is roughly 3.141672\n19/07/23 20:38:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.22:4040\n19/07/23 20:38:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n19/07/23 20:38:05 INFO MemoryStore: MemoryStore cleared\n19/07/23 20:38:05 INFO BlockManager: BlockManager stopped\n19/07/23 20:38:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n19/07/23 20:38:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n19/07/23 20:38:05 INFO SparkContext: Successfully stopped SparkContext\n19/07/23 20:38:07 INFO ShutdownHookManager: Shutdown hook called\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-63d9ecbc-7d34-4687-b91b-c9a45ad784a1/pyspark-c24060e0-db6e-4711-be4a-737b80458db1\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-63d9ecbc-7d34-4687-b91b-c9a45ad784a1\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec7af0c2-26e0-4f71-8fdc-b14a9f19f377\n```","user":"anonymous","dateUpdated":"2019-07-23T21:06:14-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Py-Spark - Spark Submit</h3>\n<pre><code>vektor@pop-os:~$ spark-submit spark/examples/src/main/python/pi.py 1000\n..............................................................................\n..............................................................................\nPi is roughly 3.141672\n19/07/23 20:38:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.22:4040\n19/07/23 20:38:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n19/07/23 20:38:05 INFO MemoryStore: MemoryStore cleared\n19/07/23 20:38:05 INFO BlockManager: BlockManager stopped\n19/07/23 20:38:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n19/07/23 20:38:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n19/07/23 20:38:05 INFO SparkContext: Successfully stopped SparkContext\n19/07/23 20:38:07 INFO ShutdownHookManager: Shutdown hook called\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-63d9ecbc-7d34-4687-b91b-c9a45ad784a1/pyspark-c24060e0-db6e-4711-be4a-737b80458db1\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-63d9ecbc-7d34-4687-b91b-c9a45ad784a1\n19/07/23 20:38:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec7af0c2-26e0-4f71-8fdc-b14a9f19f377\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563931183486_-331457077","id":"20190723-201943_1262204461","dateCreated":"2019-07-23T20:19:43-0500","dateStarted":"2019-07-23T20:41:33-0500","dateFinished":"2019-07-23T20:41:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1521"},{"user":"anonymous","dateUpdated":"2019-07-23T20:47:36-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{},"tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563932435858_727418378","id":"20190723-204035_332984812","dateCreated":"2019-07-23T20:40:35-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1522"}],"name":"/Spark_Training/Setup-Spark-Zeppelin-Locally","id":"2EGAZB37N","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}